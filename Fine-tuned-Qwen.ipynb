{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git \n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q datasets\n!pip install evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig, \n    AutoModelForSequenceClassification,\n    BitsAndBytesConfig,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer)\n\nfrom peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\nimport evaluate\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport random\nfrom transformers import EarlyStoppingCallback\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:09:08.639740Z","iopub.execute_input":"2025-04-06T13:09:08.639930Z","iopub.status.idle":"2025-04-06T13:09:16.655313Z","shell.execute_reply.started":"2025-04-06T13:09:08.639910Z","shell.execute_reply":"2025-04-06T13:09:16.654261Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load dataset\ndataset = load_dataset('google-research-datasets/go_emotions')\n\n# Concatenate training, validation and test subset to one dataset\ndataset = concatenate_datasets([dataset['train'], dataset['validation'], dataset['test']])\n\n# Choose just single label rows\ndataset = dataset.filter(lambda dic: len(dic[\"labels\"]) == 1)\n\n# Flatten the labels\ndataset = dataset.map(lambda row: {\"labels\": row[\"labels\"][0]})\n\n# Count each label\nlabel_counts = Counter(dataset['labels'])\n\n# Filter labels with more than 150 counts\nlabels_more_than_800 = [label for label, count in sorted(label_counts.items(), key=lambda item: item[1], reverse=True) if count > 800]\n\nprint(labels_more_than_800)\n\n\n# Set a fixed seed for reproducibility\nrandom.seed(11)\n\n# Preselect labels for each num_labels once\nlabel_choices = {}\nfor num_labels in [2, 4, 8, 16]:\n    if num_labels == 2:\n        label_choices[num_labels] = random.sample(labels_more_than_800[:9], num_labels)\n    else:\n        label_choices[num_labels] = random.sample(labels_more_than_800, num_labels)\n\n# Function to create balanced dataset with provided labels\ndef create_balanced_dataset(num_labels, num_samples, dataset, selected_labels):\n\n    # Create placeholders\n    train_dataset_balanced = None\n    val_dataset_balanced = None\n    test_dataset_balanced = None\n\n    for label in selected_labels:\n        single_label_dataset = dataset.filter(lambda dic: dic[\"labels\"] == label)\n\n        # Sample for train/val/test\n        n = int(num_samples / num_labels)\n        train = single_label_dataset.select(range(n))\n        val = single_label_dataset.select(range(n, int(1.1 * n)))\n        test = single_label_dataset.select(range(int(1.1 * n), int(1.2 * n)))\n\n        # Combine\n        train_dataset_balanced = train if train_dataset_balanced is None else concatenate_datasets([train_dataset_balanced, train])\n        val_dataset_balanced = val if val_dataset_balanced is None else concatenate_datasets([val_dataset_balanced, val])\n        test_dataset_balanced = test if test_dataset_balanced is None else concatenate_datasets([test_dataset_balanced, test])\n\n    return DatasetDict({\n        \"train\": train_dataset_balanced,\n        \"validation\": val_dataset_balanced,\n        \"test\": test_dataset_balanced\n    })\n\n# Now generate datasets with consistent label sets\nbalanced_datasets = {}\nfor num_labels in [2, 4, 8, 16]:\n    selected_labels = label_choices[num_labels]\n    for num_samples in [800, 1600, 2400]:\n        print(f\"Creating balanced dataset for {num_labels} labels and {num_samples} samples...\")\n        ds = create_balanced_dataset(num_labels, num_samples, dataset, selected_labels)\n\n        # Map labels to 0...num_labels-1\n        label_mapping = {old_label: new_label for new_label, old_label in enumerate(selected_labels)}\n        print('Label mapping:', label_mapping)\n        ds = ds.map(lambda example: {\"labels\": label_mapping[example[\"labels\"]]})\n        balanced_datasets[f\"{num_labels}_labels_{num_samples}\"] = ds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import metrics\naccuracy = evaluate.load(\"accuracy\")\nprecision = evaluate.load(\"precision\")\nrecall = evaluate.load(\"recall\")\nf1 = evaluate.load(\"f1\")\n\n# Define metrics computation\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=1)\n\n    return {\n        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n        \"precision\": precision.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"],\n        \"recall\": recall.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"],\n        \"f1\": f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fine_tuned_model(model_base, dataset, num_labels, num_samples):\n    \n    # Quantized the model\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,  # 4-bit quantization\n        bnb_4bit_compute_dtype=torch.float16,  # Use float16 for stability\n        bnb_4bit_use_double_quant=True  # Enable double quantization for memory efficiency\n    )\n\n    #logging.set_verbosity_error()\n\n    # generate classification model from model_checkpoint\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_base,\n        num_labels=num_labels, \n        quantization_config=bnb_config,\n        device_map= {\"\":0}\n    )\n    \n    # create tokenizer\n    tokenizer= AutoTokenizer.from_pretrained(model_base)\n    \n    # add pad token if none exists\n    if tokenizer.pad_token is None:\n        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n        model.resize_token_embeddings(len(tokenizer))\n\n    # create tokenize function\n    def tokenize_function(examples):\n        # extract text\n        text = examples[\"text\"]\n    \n        #tokenize and truncate text\n        tokenizer.truncation_side = \"left\"\n        tokenized_inputs = tokenizer(\n            text,\n            return_tensors=\"np\",\n            truncation=True,\n            max_length=512\n        )\n    \n        return tokenized_inputs\n    \n    tokenized_dataset = balanced_datasets[f\"{num_labels}_labels_{num_samples}\"].map(tokenize_function, batched=True)\n    \n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n                            r=4,\n                            lora_alpha=32,\n                            lora_dropout=0.05,\n                            target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"]\n     )\n\n    model = get_peft_model(model, peft_config)\n    \n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        eval_strategy=\"steps\",         \n        eval_steps=50,                 \n        save_strategy=\"steps\",          \n        save_steps=50,\n        logging_steps=50,\n        per_device_train_batch_size=4,  \n        per_device_eval_batch_size=4,\n        gradient_accumulation_steps=1,\n        learning_rate=1e-4,            \n        num_train_epochs=5,\n        weight_decay=0.01,\n        warmup_steps=100,              \n        lr_scheduler_type=\"cosine\",     \n        load_best_model_at_end=True,\n        metric_for_best_model=\"accuracy\",\n        fp16=False,\n        bf16=True,\n        optim=\"adamw_bnb_8bit\",\n        report_to=\"none\",\n    )\n\n\n    model.config.pad_token_id = tokenizer.pad_token_id\n    \n    early_stopping = EarlyStoppingCallback(\n        early_stopping_patience=20  \n    )\n    \n    # creater trainer object\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset[\"train\"],\n        eval_dataset=tokenized_dataset[\"validation\"],\n        tokenizer=tokenizer,\n        data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n        compute_metrics=compute_metrics,\n        callbacks=[early_stopping]\n    )\n    \n    # train model\n    trainer.train()\n\n    log_history = trainer.state.log_history\n    \n    # Separate logs by training and evaluation\n    train_logs = [log for log in log_history if 'loss' in log and 'step' in log]\n    eval_logs = [log for log in log_history if 'eval_loss' in log]\n    \n    # Convert to DataFrames\n    df_train = pd.DataFrame(train_logs)\n    df_eval = pd.DataFrame(eval_logs)\n    \n    # Merge both on step\n    df_merged = pd.merge(df_train, df_eval, on='step', how='outer')\n    \n    # Sort and reset index\n    df_merged = df_merged.sort_values(\"step\").reset_index(drop=True)\n    \n    # Rename for readability (optional)\n    df_merged.rename(columns={\n        \"loss\": \"Training Loss\",\n        \"eval_loss\": \"Validation Loss\",\n        \"eval_accuracy\": \"Accuracy\",\n        \"eval_precision\": \"Precision\",\n        \"eval_recall\": \"Recall\",\n        \"eval_f1\": \"F1\"\n    }, inplace=True)\n    \n    # Display final table\n    df_merged.to_csv(f\"results_{num_labels}_{num_samples}.csv\")\n\n    test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n    df_test_results = pd.DataFrame([test_results])\n    df_test_results[\"Dataset\"] = f\"{num_labels}_labels_{num_samples}_samples\"\n    df_test_results[\"Model\"] = model_base\n\n    return df_test_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_base= \"Qwen/Qwen2.5-1.5B\" \ndataset = balanced_datasets\n\nall_results = []\nnum_labels = 2\nfor num_labels in [2, 4, 8, 16]:\n    for num_samples in [800, 1600, 2400]:\n    \n        df = fine_tuned_model(model_base, dataset, num_labels, num_samples)\n        all_results.append(df)\n\nfinal_results = pd.concat(all_results, ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_results.to_csv('final_results.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}